<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="https://sambaiga.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sambaiga.github.io/" rel="alternate" type="text/html" /><updated>2017-11-30T21:11:01+01:00</updated><id>https://sambaiga.github.io/</id><title type="html">sambaiga</title><author><name>Anthony Faustine</name></author><entry><title type="html">Foundation of Deep learning</title><link href="https://sambaiga.github.io/ml/2017/09/11/deep-1.html" rel="alternate" type="text/html" title="Foundation of Deep learning" /><published>2017-09-11T17:12:00+02:00</published><updated>2017-10-16T07:43:12+02:00</updated><id>https://sambaiga.github.io/ml/2017/09/11/deep-1</id><content type="html" xml:base="https://sambaiga.github.io/ml/2017/09/11/deep-1.html">&lt;h2 id=&quot;introduction-to-deep-learning&quot;&gt;Introduction to deep learning&lt;/h2&gt;
&lt;p&gt;Deep Learning is a subclass of machine learning algorithms that use a cascade of multiple layers of nonlinear processing units for pattern classification and for feature or represantation learning. Unlike other machine learning, deep learning allows computationa models that are composed of multiple processing layers to learn representations of data with multiple levels of abstarction. Because of this deep learnig are flexible models with any input/output type and size.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/assets/img/post/deep.png&quot; title=&quot;Deep learning&quot; alt=&quot;deep vs machine learning&quot; /&gt;
  &lt;figcaption&gt;Deep learning @
    &lt;a href=&quot;https://www.xenonstack.com/blog/log-analytics-with-deep-learning-and-machine-learning&quot;&gt;
      xenonstack
      &lt;/a&gt;
    
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;feature-engineered-vs-feature-learning&quot;&gt;Feature engineered vs Feature learning&lt;/h3&gt;

&lt;p&gt;Machine learning algorithms use enginered feature to extract useful patterns from data that will make it easier for algorithm to learn. The process is complex and difficult since different data sets require different feature engineering approaches. In addition features that are usable for one data set often are not usable for other data sets.&lt;/p&gt;

&lt;p&gt;On the other hand deep learning models use feature learning instead of feature engineering to extract useful pattern from data. Feature learning algorithms automatically discover and extract useful pattern from data. Deep learning employ multiple level of feature learning obtained by commposing simple but non-linear modules that each transform feature at one level into a feature at higher level. This allows learning complex features such as speach and also learning complex networks.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-deep-learning-1&quot;&gt;Introduction to deep learning&lt;/h2&gt;

&lt;h2 id=&quot;the-percetron&quot;&gt;The percetron&lt;/h2&gt;

&lt;h3 id=&quot;multi-layer-percetron&quot;&gt;Multi-layer Percetron&lt;/h3&gt;

&lt;h3 id=&quot;training-multilayer-perceptrons&quot;&gt;Training Multilayer Perceptrons&lt;/h3&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Introduction to deep learning Deep Learning is a subclass of machine learning algorithms that use a cascade of multiple layers of nonlinear processing units for pattern classification and for feature or represantation learning. Unlike other machine learning, deep learning allows computationa models that are composed of multiple processing layers to learn representations of data with multiple levels of abstarction. Because of this deep learnig are flexible models with any input/output type and size. Deep learning @ xenonstack</summary></entry><entry><title type="html">Learning HMM parameters for Continous Density Models</title><link href="https://sambaiga.github.io/ml/hmm/2017/06/12/hmm-gausian.html" rel="alternate" type="text/html" title="Learning HMM parameters for Continous Density Models" /><published>2017-06-12T17:12:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/ml/hmm/2017/06/12/hmm-gausian</id><content type="html" xml:base="https://sambaiga.github.io/ml/hmm/2017/06/12/hmm-gausian.html">&lt;p&gt;In the previous post we considered a scenario in which observation sequences &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are discrete symbols. However, for many practical problems the observation symbols are continous vectors. As the results the contious probability desnsity function (pdfs) are used to model the space of the observation signal associated with each state. Most commonly used emission distribution are gaussian distribution and the gausian mixture models.&lt;/p&gt;

&lt;h3 id=&quot;gaussian-distribution-and-the-gausian-mixture-models&quot;&gt;Gaussian Distribution and the Gausian Mixture Models&lt;/h3&gt;

&lt;p&gt;It is popular to represent the randomness of continuous-valued  using the multivariate Gaussian distribution. A vector-valued random variable &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is said to have a multivariate normal (or Gaussian) distribution with mean &lt;script type=&quot;math/tex&quot;&gt;\mu=\mathop{\mathbf{E[x]}}&lt;/script&gt; and covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\Sigma=\mathbf{cov[x]}&lt;/script&gt; if:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathbf{x}; \mu, \Sigma) = \mathcal{N(\mathbf{x} \mid \mu, \Sigma)}=\frac{1}{(2\pi)^{D/2} |\Sigma|^\frac{1}{2}}\quad\exp\Big(-\frac{1}{2}[\mathbf{x} - \mu] \Sigma^{-1}[\mathbf{x} - \mu]^\mathsf{T} \Big)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; is the dimensionality of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;. The &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; represents the location where samples are most likely to be generated and the &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; indicates the level to which two variables vary together.&lt;/p&gt;

&lt;p&gt;However, a single Gaussian distribution is insufficient to represent the state-dependent observation space for an HMM state &lt;script type=&quot;math/tex&quot;&gt;s_t=i&lt;/script&gt; because there are large amounts of training data collected from various appliance instances with different modes, distortions, background noises, etc which are used to train the parameters of individual HMM states. In this case, a Gaussian mixture model (GMM) is adopted to represent the state-dependent observation space.&lt;/p&gt;

&lt;p&gt;A mixture model is a probabilistic model for density estimation using a mixture distribution and can be regarded as a type of unsupervised learning or clustering. They provide a method of describing more complex propability distributions, by combining several probability distributions. A multivariate Gaussian mixture distribution is given by the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathbf{x}) = \displaystyle\sum_{k=1}^{K}\omega_k \mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)}&lt;/script&gt;

&lt;p&gt;The parameters &lt;script type=&quot;math/tex&quot;&gt;\omega_k&lt;/script&gt; are called mixing coefficients, which must fulfill&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle\sum_{k=1}^{K}\omega_k =1&lt;/script&gt;

&lt;p&gt;and given &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)} \geq 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P(\mathbf{x}) \geq 0&lt;/script&gt; we also have that 
&lt;script type=&quot;math/tex&quot;&gt;0\leq \omega_k \geq 1&lt;/script&gt;. Each Gaussian density &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)}&lt;/script&gt; is
called a component of the mixture and has its own mean &lt;script type=&quot;math/tex&quot;&gt;\mu_k&lt;/script&gt;   and covariance &lt;script type=&quot;math/tex&quot;&gt;\Sigma_k&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hmm-with-gaussian-emission-distribution&quot;&gt;HMM with gaussian emission distribution&lt;/h3&gt;

&lt;p&gt;If the observations are continuous, it is common for the emission probabilities to be a conditional
Gaussian such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathbb{y_t} \mid s_t =i) = \mathcal{N(\mathbf{y_t} \mid \mu_i, \Sigma_i)}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mu_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma_i&lt;/script&gt; are mean vector and covariance matrix associated with state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The re-estimation formula for the mean vector and covariance matrix of a state gausian pdf can be derived as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \hat{\mu}_i &amp; =\frac{\displaystyle\sum_{t=1}^{T}\gamma_t(i)\mathbb{y(t)}}{\displaystyle\sum_{t=1}^{T}\gamma _t(i)}\\
 \hat{\Sigma}_i &amp; =\frac{\displaystyle\sum_{t=1}^{T}\gamma_t(i) [\mathbf{y(t)}-\hat{\mu}_i]\cdot[\mathbf{y(t)}-\hat{\mu}_i]^T}{\displaystyle\sum_{t=1}^{T}\gamma_t(i)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;hmms-with-gaussian-mixture-model&quot;&gt;HMMs with Gaussian Mixture Model&lt;/h3&gt;

&lt;p&gt;In HMMs with gaussian mixture pdf, the emission probabilities is given by&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\mathbb{y_t} \mid s_t =i) = \displaystyle\sum_{k=1}^{M} \omega\_{ik}\mathcal{N(\mathbb{y_t} \mid \mu_{ik}, \Sigma_{ik})}&lt;/script&gt;
  where &lt;script type=&quot;math/tex&quot;&gt;\omega_{ik}&lt;/script&gt; is the prior probability of the  &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; component of the mixture.&lt;/p&gt;

&lt;p&gt;The posterior probability of state &lt;script type=&quot;math/tex&quot;&gt;s_t=i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and state &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}=j&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t+1&lt;/script&gt; given the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and the observation sequence &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \gamma_t(i,j)&amp; =P(s_t=i, s_{t+1}=j \mid Y, \lambda) \\
 &amp; = \frac{\alpha_t(i)a_{ij}\Big[ \displaystyle\sum_{k=1}^{M} \omega_{ik}\mathcal{N(\mathbf{y_t} \mid \mu_{ik}, \Sigma_{ik})} \Big]\beta_{t+1}(j)}{\displaystyle\sum_{i=1}^{N}\alpha_T(i)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the posterior probability of state &lt;script type=&quot;math/tex&quot;&gt;s_t=i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; given the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and observation &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_t(i) =\frac{\alpha_t(i)\beta_t(i)}{\displaystyle\sum_{i=1}^{N}\alpha _T(i)}&lt;/script&gt;

&lt;p&gt;Let define the joint posterior probability of the state &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; and the &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; gaussian component pdf of state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\xi(i,k) &amp;= P(S_t=s_i, m(t)=k \mid Y, \lambda) \\
 &amp;=\frac{\displaystyle\sum_{j=1}^{N} \alpha_t(j) a_{ij} \omega_{ik}\mathcal{N(\mathbf{y_t} \mid \mu_{ik}, \Sigma_{ik})}\beta_{t+1}(j)}{\displaystyle\sum_{i=1}^{N}\alpha _T(i)} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The re-estimation formula for the mixture coefficeints, the mean vectors and the covariance matrices of the state mixture gausian pdf as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \hat{\omega}_{ik} &amp;= \frac{\displaystyle \sum_{t=1}^{T} \xi_t(i,k)}{\displaystyle\sum\_{t=0}^{T}\gamma_t(i)} \\
\hat{\mu}_{ik} &amp;= \frac{\displaystyle\sum\ _{t=1}^{T}\xi\ _t(i,k)\mathbf{y_t}}{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)} \\
\hat{\Sigma}_{ik}&amp;=\frac{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)[\mathbf{y_t}-\hat{\mu}_{ik}]\cdot[\mathbf{y_t}-\hat{\mu}_{ik}]^T}{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;limitation-of-baumwelch-algorithm&quot;&gt;Limitation of Baum–Welch algorithm&lt;/h3&gt;

&lt;p&gt;When applying Baum–Welch algorithm  in real  data, we need to consider some heuristics in the ML EM algorithm.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How to provide initial parameters values. This is always an important question, and it is usually resolved by using a simple algorithm (e.g., K-means clustering or random initialization).&lt;/li&gt;
  &lt;li&gt;How to avoid unstability in the parameter estimation (especially covariance parameter estimation) due to data sparseness. For examle some mixture components or hidden states cannot have sufficient data assigned in the Viterbi or forward–backward algorithm. This can be heuristically avoided by setting a threshold to update these parameters, or setting minimum threshold values for covariance parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The above two problem can be solved by the Bayesian approaches.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Saeed V. Vaseghi, Advanced Digital Signal Processing and Noise Reduction. John Wiley &amp;amp; Sons, 2008.&lt;/li&gt;
  &lt;li&gt;Kevin P. Murphy, Machine Learning: A Probabilistic Perspective. The MIT Press Cambridge, Massachusetts, 2012.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">In the previous post we considered a scenario in which observation sequences are discrete symbols. However, for many practical problems the observation symbols are continous vectors. As the results the contious probability desnsity function (pdfs) are used to model the space of the observation signal associated with each state. Most commonly used emission distribution are gaussian distribution and the gausian mixture models.</summary></entry><entry><title type="html">Learning HMM parameters with Discrete Observation Models</title><link href="https://sambaiga.github.io/ml/hmm/2017/05/29/hmm-discrete.html" rel="alternate" type="text/html" title="Learning HMM parameters with Discrete Observation Models" /><published>2017-05-29T17:12:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/ml/hmm/2017/05/29/hmm-discrete</id><content type="html" xml:base="https://sambaiga.github.io/ml/hmm/2017/05/29/hmm-discrete.html">&lt;p&gt;In Previous post we discussed the basic of HMM modeling given model parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and  compute the likelihood values etc, efficiently based on the forward, backward, and Viterbi algorithms. In the like manner, we can efficiently train the HMM to obtain the model parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}&lt;/script&gt; from data. In this post we will discuss different methods for training HMM models.&lt;/p&gt;

&lt;p&gt;This is the solution to Problem 3 which involve determining a method to learn model parameters &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}&lt;/script&gt; given the sequence of observation variables &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;. Given the observation sequences &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as training data, there is no optimal way of estimating the model parametrs. However, using iterative procedure we can choose &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda} = (\hat{A},\hat{B},\hat{\pi})&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt; is locally maximized.The most common produre which has been employed to his problem is the &lt;strong&gt;Baum-Welch&lt;/strong&gt; method.&lt;/p&gt;

&lt;h3 id=&quot;baum-welch-methods&quot;&gt;Baum-Welch Methods&lt;/h3&gt;

&lt;p&gt;This method assume an initial model parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; which should be adjusted so as to increase &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt;. The initial parametrs can be constructed in any way or employ the first five procedure of the &lt;a href=&quot;http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/segmental%20k-means%20algorithm.pdf&quot;&gt;Segmental K-means algorithm&lt;/a&gt;. The optimazation criteria is called the &lt;strong&gt;maximum likelihood criteria&lt;/strong&gt;.The function &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt; is called the &lt;strong&gt;likelihood function&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-e-m-auxilliary-function&quot;&gt;The E-M Auxilliary Function&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; represent the current model and &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}&lt;/script&gt; represent the candidate models. The learning objective is to make: &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \hat{\lambda}) \geq P(Y \mid \lambda)&lt;/script&gt; which is equivalently to &lt;script type=&quot;math/tex&quot;&gt;\log[P(Y \mid \hat{\lambda})] \geq \log [P(Y\mid \lambda)]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Let also define the auxilliary function &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\lambda}\mid \lambda)&lt;/script&gt; such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q(\hat{\lambda}\mid \lambda) &amp; = \mathbb{E}\Big[\log P(Y,S \mid \hat{\lambda})\mid Y, \lambda \Big] \\
                            &amp; = \sum_s P(S \mid Y, \lambda)\cdot \log [P(Y,S\mid \hat{\lambda})]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Maximum Likehood Estimation (MLE) of the model parameter &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; for complete data &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and hidden state &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\lambda} = \arg\max _{\lambda} \sum_s P(Y, S \mid \lambda)&lt;/script&gt;

&lt;p&gt;However due to the presence of several stochatsic constraints it turns out to be easier to mximize uxilliary function &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\lambda}\mid \lambda)&lt;/script&gt; rather than directly maximize &lt;script type=&quot;math/tex&quot;&gt;P(Y\mid \hat{\lambda})&lt;/script&gt;. Thus the MLE of the model parameter &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; for complete data &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and hidden state &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\lambda} = \arg\max _{\lambda} Q(\hat{\lambda}\mid\lambda)&lt;/script&gt;

&lt;p&gt;It can be shown that the parameter estimated by the EM procedure, &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\lambda}\mid \lambda)&lt;/script&gt;, always increases the likelihood value. You may concert &lt;a href=&quot;https://books.google.co.tz/books/about/Bayesian_Speech_and_Language_Processing.html?id=rEzzCQAAQBAJ&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button&amp;amp;redir_esc=y#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;reference 2&lt;/a&gt; chapter 3 for details on the prove.&lt;/p&gt;

&lt;h3 id=&quot;expectation-step&quot;&gt;Expectation step&lt;/h3&gt;

&lt;p&gt;To find ML estimates of HMM parameters, we first expand the auxiliary function rewrite it by substituting the joint distribution of complete data likelihood.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q(\hat{\lambda} \mid \lambda) &amp; = \mathbf{E}\Big[\log P(Y,S\mid\hat{\lambda})\mid Y, \lambda \Big] \\
&amp; = \sum_s P(S\mid Y, \lambda)\cdot \log [P(Y,S\mid\hat{\lambda})] \\
&amp; = \sum_s P(S\mid Y, \lambda)\cdot \Big[\log \hat{\pi}_1 + \log \hat{b}_1(y_1) + \sum _{t=2}^T\big( \log \hat{a} _{ij} + \log \hat{b}_i({y_t})\big)\Big] 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have three term to solve:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The initial probability &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}&lt;/script&gt; ,&lt;/li&gt;
  &lt;li&gt;State transition probability &lt;script type=&quot;math/tex&quot;&gt;\hat{A} = \hat{a}_{ij}&lt;/script&gt; and&lt;/li&gt;
  &lt;li&gt;Emission probability &lt;script type=&quot;math/tex&quot;&gt;\hat{B} = \hat{b}_i(y_t)&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let first define important parameters that we will use. For &lt;script type=&quot;math/tex&quot;&gt;t = 1,2...T&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;1\leq i \geq N&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1\leq j \geq N&lt;/script&gt;, we define:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi_t(i,j)=P(s_t=i, s_{t+1}=j \mid Y, \lambda)&lt;/script&gt;

&lt;p&gt;an expected transition probability from &lt;script type=&quot;math/tex&quot;&gt;s_t=i&lt;/script&gt; to , &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}=j&lt;/script&gt;. The probability of being in state &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and state &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t+1&lt;/script&gt; given the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and observation sequences &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\xi_t(i,j)&lt;/script&gt; can be written in terms of forward &lt;script type=&quot;math/tex&quot;&gt;\alpha_t(i)&lt;/script&gt; and backward &lt;script type=&quot;math/tex&quot;&gt;\beta_{t+1}(j)&lt;/script&gt; variables as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}   
\xi_t(i,j) &amp;= \frac{\alpha_t(i)a_{ij}b_i(y_{t+1})\beta_{t+1}(j)}{P(Y \mid \lambda)} \\ 
          &amp;= \frac{\alpha_t(i)a_{ij}b_i(y_{t+1})\beta_{t+1}(i)}{\displaystyle \sum_{i=1}^{N}\displaystyle \sum_{j=1}^{N}\alpha_t(i)a_{ij}b_j(y_{t+1})\beta_{t+1}(j)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the numerator term is just &lt;script type=&quot;math/tex&quot;&gt;P(S_t=s_i, S_{t+1}=s_j \mid Y, \lambda)&lt;/script&gt;  and the division by &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt; gives the desire probability measures.&lt;/p&gt;

&lt;p&gt;We have previosly difined &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i) =  \frac{\alpha_t(i)\beta_t(i)}{P(Y \mid \lambda)}&lt;/script&gt; as the probability of being in state &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; given the observation sequence and model parameter. &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i)&lt;/script&gt; relate to &lt;script type=&quot;math/tex&quot;&gt;\xi_t(i,j)&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_t(i) = \displaystyle\sum_{j=1}^{N}\xi_t(i,j)&lt;/script&gt;

&lt;p&gt;It follows that:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\displaystyle\sum_{t=1}^{T-1}\gamma_t(i)=&lt;/script&gt; Expected number of transitions from state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; .&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\displaystyle\sum_{t=1}^{T-1}\xi_t(i,j)=&lt;/script&gt; Expected number of transitions from state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; to state &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We provide the solution for each term. Considering the first term  &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\pi} \mid \pi)&lt;/script&gt; we define the following auxiliary function for &lt;script type=&quot;math/tex&quot;&gt;\pi _i&lt;/script&gt;  as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\hat{\pi}\mid \pi) = \sum_s P(S\mid Y, \lambda)\cdot \log \hat{\pi}_{s_1}&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_{s_1}&lt;/script&gt; only depends on &lt;script type=&quot;math/tex&quot;&gt;s_1&lt;/script&gt;, it clear that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(S\mid Y, \lambda) = P(s_1\mid Y, \lambda)&lt;/script&gt;

&lt;p&gt;Therefore &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\pi}\mid pi)&lt;/script&gt;  can be rewritten as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q(\hat{\pi}\mid \pi) &amp;= \sum_{s_1}P(s_1 \mid Y, \lambda)\cdot \log \hat{\pi}_{s_1} \\
                     &amp;= \sum_{i=1}^N P(s_1=i \mid Y, \lambda)\cdot \log \hat{\pi}_{i} \\
                     &amp; = \sum_{i=1}^N \gamma_t(i) \log \hat{\pi}_{i}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Next, we focus on the second term &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{A}\mid A)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\hat{A}\mid A) = \sum_s P(S\mid Y, \lambda) \cdot \sum _{t=2}^T  \log \hat{a}_{s_t,s_{t+1}}&lt;/script&gt;

&lt;p&gt;Similar to &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\pi}\mid \pi)&lt;/script&gt; , we obtain
&lt;script type=&quot;math/tex&quot;&gt;P(S\mid Y, \lambda) = P(s_1\mid Y, \lambda) = P(s_t,s_{t+1}\mid Y, \lambda)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q(\hat{A}\mid A) &amp; =  \sum _{t=1}^{T-1}  \sum_s P(s_t,s_{t+1}\mid Y, \lambda) \log \hat{a}_{s_t,s_{t+1}} \\
 &amp; = \sum _{t=1}^{T-1} \sum_{i=1}^N \sum_{j=1}^N P(s_t=i,s_{t+1}=j\mid Y, \lambda) \log \hat{a}_{ij} \\
&amp; = \sum _{t=1}^{T-1} \sum_{i=1}^N \sum_{j=1}^N \xi_t(i,j) \log \hat{a}_{ij}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finally, we focus on the last term &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{B}\mid B)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\hat{B}\mid B) = \sum_s P(S\mid Y, \lambda)\cdot \sum _{t=1}^T  \log \hat{b}_{i}(y_t)&lt;/script&gt;

&lt;p&gt;Similary &lt;script type=&quot;math/tex&quot;&gt;P(S\mid Y, \lambda) = P(s_t = i\mid Y, \lambda)&lt;/script&gt;. Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q(\hat{B}\mid B) &amp;= \sum _{t=1}^T \sum_s P(s_t = i\mid Y, \lambda) \log \hat{b}_{i}(y_t) \\
&amp; = \sum _{t=1}^T \sum_{i=1}^N \gamma_t(i)\log \hat{b}_{i}(y_t)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, we summarize the auxiliary function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\hat{\lambda}\mid \lambda)= Q(\hat{\pi}\mid \pi)+ Q(\hat{A}\mid A) + Q(\hat{B}\mid B)&lt;/script&gt;

&lt;h3 id=&quot;maximization-step&quot;&gt;Maximization step&lt;/h3&gt;

&lt;p&gt;In the maximization step, we aim to maximize &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{\pi} \mid \pi)&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{A} \mid A)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q(\hat{B} \mid B)&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\hat{A}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{B}&lt;/script&gt; under the following constraints.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\_{i=1}^N \hat{\pi} = 1, \text{ and } \sum_{i=1}^N \hat{A} = 1&lt;/script&gt;

&lt;p&gt;Considering the estimation of initial state probabilities &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\hat{\pi}} = {\hat{\pi}_i}&lt;/script&gt; , we construct a Lagrange function (or Lagrangian):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^*(\hat{\pi} \mid \pi) =\sum_{i=1}^N \gamma_1(i) \log \hat{\pi}_{i} + \eta \left(\sum_{i=1}^N \hat{\pi} - 1 \right)&lt;/script&gt;

&lt;p&gt;Differentiating this Lagrangian with respect to individual probability parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_i&lt;/script&gt;  and set it to zero we obtain.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial Q^*(\hat{\pi} \mid \pi)}{\partial \hat{\pi}_i } &amp; = \gamma_1(i) \frac{1}{\hat{\pi}_i} + \eta = 0 \\
\hat{\pi}_i &amp;=  - \frac{1}{\eta}\gamma_1(i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Substituting the above equation into &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N \hat{\pi} = 1&lt;/script&gt; constraint, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sum_{i=1}^N \hat{\pi} &amp;= \sum_{i=1}^N - \frac{1}{\eta}\gamma_1(i) = 1 \\
\Rightarrow \eta &amp;= - \sum_{i=1}^N \gamma_1(i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The ML estimate of new initial state probability is obtained by substituting the above equation into &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_i =  - \frac{1}{\eta}\gamma_1(i)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\pi}_i = \frac{\gamma_1(i)}{\sum _{i=1}^N \gamma_1(i)} = \gamma_1(i)&lt;/script&gt;

&lt;p&gt;In the same manner, we can derive the ML estimates of new state transition probability and new emission probability, which can be shown to be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{a}_{ij} = \frac{\displaystyle \sum_{t=1}^{T-1}\xi_t(i,j)}{\displaystyle\sum_{t=1}^{T-1}\gamma_t(i)}&lt;/script&gt;

&lt;p&gt;And&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{b}_i(k) = \frac{\displaystyle\sum_{t=1}^{T}\tau \gamma_t(i)}{\displaystyle\sum_{t=1}^{T} \gamma_t(i)}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau =
 \begin{cases}
1 \text{ if } y_t = k, \\ 0  \text{ otherwise }
\end{cases}&lt;/script&gt;

&lt;p&gt;If we denote the initial model &lt;script type=&quot;math/tex&quot;&gt;{\lambda}&lt;/script&gt; and the re-estimation model by &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}=(\hat{\pi}_i, \hat{a}_{ij},\hat{b}_j(k))&lt;/script&gt;. Then i t can be shown that either:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The initial model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is a critical point of the likelihood in which case &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}= \lambda&lt;/script&gt; or&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \hat\lambda) \leq P(Y \mid \lambda)&lt;/script&gt;, i.e we have find the better model from which the observation sequence &lt;script type=&quot;math/tex&quot;&gt;Y=y_1,\ldots Y_T&lt;/script&gt; is more likely to be produced.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hence we can go on iteractively computing until &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \hat{\lambda})&lt;/script&gt; is maximazed.&lt;/p&gt;

&lt;p&gt;The Baum-Welch Algorithm can be summerized as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Require&lt;/strong&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lambda \leftarrow \lambda ^{init}&lt;/script&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt; &lt;strong&gt;repeat&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;  Compute the forward variable &lt;script type=&quot;math/tex&quot;&gt;\alpha _t(i)&lt;/script&gt; from the forward algorithm&lt;/li&gt;
  &lt;li&gt;  Compute the backward variable &lt;script type=&quot;math/tex&quot;&gt;\beta _t(i)&lt;/script&gt; from the backward algorithm&lt;/li&gt;
  &lt;li&gt;  Compute the occupation probabilities &lt;script type=&quot;math/tex&quot;&gt;\gamma _t(i)&lt;/script&gt;,  and &lt;script type=&quot;math/tex&quot;&gt;\xi _t(i,j)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;  Estimate the new HMM parameters &lt;script type=&quot;math/tex&quot;&gt;\hat{\lambda}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;  Update the HMM parameters   &lt;script type=&quot;math/tex&quot;&gt;\lambda \leftarrow \hat{\lambda}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt; &lt;strong&gt;until&lt;/strong&gt; Convergence&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;L. R. Rabiner, &lt;a href=&quot;http://www.cs.ucsb.edu/~cs281b/papers/HMMs%20-%20Rabiner.pdf&quot;&gt;A tutorial on hidden Markov models and selected applications in speech recognition&lt;/a&gt;, Proceedings of the IEEE, Vol. 77, No. 2, February 1989.&lt;/li&gt;
  &lt;li&gt;Shinji Watanabe, Jen-Tzung Chien, &lt;a href=&quot;https://books.google.co.tz/books/about/Bayesian_Speech_and_Language_Processing.html?id=rEzzCQAAQBAJ&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button&amp;amp;redir_esc=y#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;Bayesian Speech and Language Processing&lt;/a&gt;, Cambridge University Press, 2015.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/&quot;&gt;Viterbi Algorithm in Speech Enhancement and HMM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Nikolai Shokhirev, &lt;a href=&quot;http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html&quot;&gt;Hidden Markov Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">In Previous post we discussed the basic of HMM modeling given model parameters and compute the likelihood values etc, efficiently based on the forward, backward, and Viterbi algorithms. In the like manner, we can efficiently train the HMM to obtain the model parameter from data. In this post we will discuss different methods for training HMM models.</summary></entry><entry><title type="html">The Basic of Hidden Markov Model</title><link href="https://sambaiga.github.io/2017/05/03/hmm-intro.html" rel="alternate" type="text/html" title="The Basic of Hidden Markov Model" /><published>2017-05-03T00:00:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/2017/05/03/hmm-intro</id><content type="html" xml:base="https://sambaiga.github.io/2017/05/03/hmm-intro.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;HMM is a Markov model whose states are not directly observed instead each state is characterised by a probability distribution function modelling the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM.&lt;/p&gt;

&lt;p&gt;There are two variables in HMM: observed variables and hidden variables where the sequences of hidden variables forms a Markov process as shown in figure below. In the context of NILM, the hidden variables are used to model states(ON,OFF, standby etc) of individual appliances and the observed variables are used to model the electric usage. HMMs has been widely used in most of the recently proposed NILM approach because it represents well the individual appliance internal states which are not directly observed in the targeted energy consumption.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/assets/img/post/hmm.png&quot; title=&quot;HMM graphical model&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;HMM graphical model
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A typical HMM is characterised by the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The finite set of hidden states  &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; (e.g ON, stand-by, OFF, etc.) of an appliance,  &lt;script type=&quot;math/tex&quot;&gt;S = \{s_1, s_2....,s_N\}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The finite set of  &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; observable symbol  &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; per states (power consumption) observed in each state,  &lt;script type=&quot;math/tex&quot;&gt;Y = \{y_1, y_2....,y_M\}&lt;/script&gt;. The observable symbol  &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; can be discrete or a continuous set.&lt;/li&gt;
  &lt;li&gt;The transition matrix   &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}=\{a_{ij},1\leq i,j \geq N\}&lt;/script&gt; represents the probability of moving from state  &lt;script type=&quot;math/tex&quot;&gt;s_{t-1}=i&lt;/script&gt; to  &lt;script type=&quot;math/tex&quot;&gt;s_t =j&lt;/script&gt; such that:  &lt;script type=&quot;math/tex&quot;&gt;a_{ij} = P(s_{t} =j \mid s_{t-1}=i)&lt;/script&gt;, with  &lt;script type=&quot;math/tex&quot;&gt;a_{ij} \leq 0&lt;/script&gt; and where  &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; denotes the state occupied by the system at time  &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The matrix  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt; is  &lt;script type=&quot;math/tex&quot;&gt;N x N&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The emission matrix  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{B} =\{b_j(k)\}&lt;/script&gt; representing the probability of emission of symbol  &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; when system state is  &lt;script type=&quot;math/tex&quot;&gt;s_t=j&lt;/script&gt; such that:  &lt;script type=&quot;math/tex&quot;&gt;b_j(k) = p(y_t = k  \mid  s_t=j)&lt;/script&gt; The matrix  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}&lt;/script&gt; is an  &lt;script type=&quot;math/tex&quot;&gt;N x M&lt;/script&gt;. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission.&lt;/li&gt;
  &lt;li&gt;And the initial state probability distribution  &lt;script type=&quot;math/tex&quot;&gt;\bold{\pi}  = \{\pi_i \}&lt;/script&gt; indicating the probability of each state of the hidden variable  at  &lt;script type=&quot;math/tex&quot;&gt;t = 1&lt;/script&gt; such that,  &lt;script type=&quot;math/tex&quot;&gt;\pi _i = P(q_1 = s_i), 1 \leq i \geq N&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The complete HMM specification requires;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Finite set of hidden states &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; and observation symbols &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Length of observation seqences &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and&lt;/li&gt;
  &lt;li&gt;Specification of three probability measures &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}, \mathbf{B}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\pi}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The set of all HMM model parameters is represented by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\lambda} =(\pi, A, B)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is not observed, the likelihood function &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is given by the joint distribution of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; over all possible state.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y \mid \lambda) = \sum P(Y, S \mid  \lambda)&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y,S \mid \lambda) = P(Y \mid S,\lambda)P(S \mid \lambda)&lt;/script&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;y_t&lt;/script&gt; is independent and identically distributed given state sequence &lt;script type=&quot;math/tex&quot;&gt;S = \{s_1, s_2....,s_N\}&lt;/script&gt;. Also each state at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; depend on the state at its previous time &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y \mid S, \lambda) = \prod_{t=1}^T P(y_t \mid s_t)&lt;/script&gt;

&lt;p&gt;Similary&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(S \mid \lambda) = \pi _{s_1} \prod _{t=2}^T a_{ij}&lt;/script&gt;

&lt;p&gt;The joint probability is therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y \mid \lambda) = \pi _{s_1}P(y_1 \mid s_1) \sum \prod_{t=2}^T a_{ij} P(y_t \mid s_t)&lt;/script&gt;

&lt;h2 id=&quot;three-main-problems-in-hmms&quot;&gt;Three main problems in HMMs&lt;/h2&gt;

&lt;p&gt;When applying HMM to a real world problem, three important problem must be solved.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Evaluation Problem: Given HMM parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and the observation seqence &lt;script type=&quot;math/tex&quot;&gt;Y = \{Y_1, Y_2....,Y_M\}&lt;/script&gt;, find &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt; the likelihood of the observation sequence &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; given the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. This problem give a score on how well a given model matches a given observation and thus allows you to choose the model that best match the observation.&lt;/li&gt;
  &lt;li&gt;Decoding Problem: Given HMM parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and the observation seqence &lt;script type=&quot;math/tex&quot;&gt;Y = \{Y_1, Y_2....,Y_M\}&lt;/script&gt;, find an optimal state sequense &lt;script type=&quot;math/tex&quot;&gt;S = \{S_1, S_2....,S_N\}&lt;/script&gt; which best explain the observation.This problem attempt to cover the hidden part of the model.&lt;/li&gt;
  &lt;li&gt;Learning Problem: Given the obseravtion seqence &lt;script type=&quot;math/tex&quot;&gt;Y = \{Y_1, Y_2....,Y_M\}&lt;/script&gt;, find the model parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that maximize &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt;.This problem attempt to optimize the model parameters so as to describe the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post.&lt;/p&gt;

&lt;h2 id=&quot;solution-to-problem-1&quot;&gt;Solution to Problem 1&lt;/h2&gt;

&lt;p&gt;A straight forward way to solve this problem is to find &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid S, \lambda)&lt;/script&gt; for fixed state sequences &lt;script type=&quot;math/tex&quot;&gt;S = \{s_1,...s_T \}&lt;/script&gt; and then sum up over all possible states. This is generally infeasible since it requires about &lt;script type=&quot;math/tex&quot;&gt;2TN^T&lt;/script&gt; multiplications. However this problem can be efficiently solved by using the forward algorithm  as follows:&lt;/p&gt;

&lt;h3 id=&quot;the-forward-backward-algorithm&quot;&gt;The forward-backward Algorithm&lt;/h3&gt;

&lt;p&gt;Let us define the &lt;strong&gt;forward variable&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha _t(i)=P(y_1,\ldots y_t, s_t=i \mid \lambda)&lt;/script&gt;

&lt;p&gt;the probability of the partial observation sequences &lt;script type=&quot;math/tex&quot;&gt;y_1 \ldots y_t&lt;/script&gt;  up to time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and the state &lt;script type=&quot;math/tex&quot;&gt;s_t =i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; given the model &lt;script type=&quot;math/tex&quot;&gt;{\lambda}&lt;/script&gt;. We also define an emission probability given HMM state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;b_i(y_t)&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;forward-algorithm&quot;&gt;Forward-Algorithm&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Initilization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\alpha _1(i)&amp;=P(y_1, s_1=i \mid \lambda) \\
    &amp; = P(y_1 \mid s_1=i,\lambda)P(s_1=i \mid \lambda)\\
    &amp;= \pi _i b_i(y_1) \text{  for  } 1\leq i \geq N
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Induction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;t=2,3...T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1\leq i \geq N&lt;/script&gt;, compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\alpha _{t}(i) &amp; = P(y_1 \ldots y_t, s_t=i \mid \lambda)\\
 &amp;= \displaystyle \sum_{j=1}^{N} P(y_1 \ldots y_{t}, s_{t-1}=j,s_t=i \mid \lambda) \\
 &amp;= \displaystyle \sum_{j=1}^{N} P(y_t \mid s_t=i, y_1,\ldots y_{t-1}, s_{t-1}=j, \lambda) \\
   &amp;  \times P(s_t=i \mid y_1 \ldots y_{t-1} \ldots , s_{t-1}=j, \lambda) \\
   &amp; \times P(y_1 \ldots y_{t-1}, s_{t-1}=j,\lambda) \\
 &amp; = P(y_t \mid s_t=i,\lambda)\displaystyle \sum_{j=1}^{N} P(s_t=i \mid s_{t-1}=j)\cdot P(y_1, \ldots y_{t-1}, s_{t-1}) \\
&amp; = b_i(y_{t})\displaystyle \sum_{j=1}^{N} \alpha _{t-1}(i)a_{ij}  
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Termination&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From &lt;script type=&quot;math/tex&quot;&gt;\alpha _t(i)=P(y_1,...y_t, s_t=i \mid \lambda)&lt;/script&gt;, it cear that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
P(Y \mid \lambda) &amp;= \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_T = i \mid \lambda) \\
&amp;= \displaystyle \sum_{i=1}^{N}\alpha _T(i)  
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The forward algorithm only requires about &lt;script type=&quot;math/tex&quot;&gt;N^2T&lt;/script&gt; multiplications and is it can be implemented in python as follows.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# returns log P(Y  \mid  model)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# using the forward part of the forward-backward algorithm&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;      
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;backward-algorithm&quot;&gt;Backward Algorithm&lt;/h3&gt;

&lt;p&gt;This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the &lt;strong&gt;backward variable&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\beta_t(i)=P(y_{t+1},y_{t+2} \ldots y_{T} \mid s_t=i, {\lambda})&lt;/script&gt;: probability of the partial observed sequence from &lt;script type=&quot;math/tex&quot;&gt;t+1&lt;/script&gt; to the end at &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; given state &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Then &lt;script type=&quot;math/tex&quot;&gt;\beta_t(i)&lt;/script&gt; can be computed recursively as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initilization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\beta_{T}(i)= 1&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;1 \leq i\geq N&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Induction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;t =T-1, T-2,\ldots1&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;1 \leq i\geq N&lt;/script&gt; and by using the sum and product rules, we can rewrite &lt;script type=&quot;math/tex&quot;&gt;\beta_t(j)&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\beta_t(i)&amp;=P(y_{t+1},\ldots y_{T} \mid s_t=j, {\lambda}) \\
 &amp;= \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i \mid s_t=j, \lambda) \\
 &amp; = \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i, s_t=j, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
 &amp;= \displaystyle \sum_{i=1}^{N} P(y_{t+2} \ldots y_T, s_{t+1}=i, \lambda)\cdot P(y_{t+1} \mid s_{t + 1}=i, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
 &amp; = \displaystyle \sum_{i=1}^{N} a_{ij}b_i(y_{t+1})\beta _{t+1}(i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Termination&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\beta_{0} &amp; = P(Y \mid \lambda) \\
&amp; = \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_1=i) \\
&amp;= \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
&amp; = \displaystyle \sum_{i=1}^{N} P(y_1 \mid s_1=i)\cdot P(y_2,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
&amp; = \displaystyle \sum_{i=1}^{N} \pi _i b_i(y_1)\beta _1(i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Python implementation of forward algorithm is as shown below;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;posterior-probability&quot;&gt;Posterior Probability&lt;/h4&gt;
&lt;p&gt;The forward variable &lt;script type=&quot;math/tex&quot;&gt;\alpha _t(i)&lt;/script&gt; and backward variable &lt;script type=&quot;math/tex&quot;&gt;\beta _t(i)&lt;/script&gt; are used to calculate the posterior probability of a specific case. Now for &lt;script type=&quot;math/tex&quot;&gt;t=1...T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i=1..N&lt;/script&gt;, let define posterior probability &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i)=P(s_t=i \mid Y, \lambda)&lt;/script&gt; the probability of being in state &lt;script type=&quot;math/tex&quot;&gt;s_t = i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; given the observation &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and the model &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\gamma_t(i) &amp; = \frac{P(s_t=1, Y \mid \lambda)}{P(Y \mid \lambda)} \\
 &amp;=\frac{P(y_1,\ldots y_t, s_t=1, \mid \lambda)}{P(Y \mid \lambda)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Consider:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P(y_1,\ldots y_t, s_t=1, \mid \lambda) &amp; = P(y_1,\ldots y_t \mid  s_t=1,\lambda)\cdot P(y_{t+1},\ldots y_T \mid  s_t=1,\lambda)\cdot P(s_t =i  \mid \lambda) \\
 &amp; = \alpha _t(i) \cdot \beta _t(i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_t(i) = \frac{\alpha _t(i) \cdot \beta _t(i)}{P(Y \mid \lambda)}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y \mid {\lambda}) =  \displaystyle \sum_{i=1}^{N}\alpha _T(i)&lt;/script&gt;

&lt;p&gt;In python:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;obs_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can use &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i)&lt;/script&gt; to find the most likely state at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; which is the state &lt;script type=&quot;math/tex&quot;&gt;s_t=i&lt;/script&gt; for which &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i)&lt;/script&gt; is maximum. This algorithm &lt;a href=&quot;http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html&quot;&gt;works fine in the case when HMM is ergodic&lt;/a&gt; i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem &lt;em&gt;Viterbi algorithm&lt;/em&gt; is the most common decoding algorithms used.&lt;/p&gt;

&lt;h3 id=&quot;viterbi-algorithm&quot;&gt;Viterbi Algorithm&lt;/h3&gt;

&lt;p&gt;Viterbi is a kind of dynamic programming algorithm that make uses of a dynamic programming trellis.&lt;/p&gt;

&lt;p&gt;The virtebi algorithm offer an efficient way of finding  the single best state sequence.Let define the highest probability along a single path, at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, which accounts for the first &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; observations and ends in state &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; using a new notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta_t(i) &amp; = \max_{s_1,\ldots s_{t-1}} P(s_1, \ldots s_t =1, y_1,\ldots y_t \mid \lambda)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By induction, a recursive formula of &lt;script type=&quot;math/tex&quot;&gt;\delta_{t+1}(i)&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;\delta_t(i)&lt;/script&gt;  is derived to calculate this probability as follows:&lt;/p&gt;

&lt;p&gt;Consider the joint distribution appearing in &lt;script type=&quot;math/tex&quot;&gt;\delta_{t+1}(i)&lt;/script&gt;, which can be rewritten when &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}=i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s_t = j&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P(s_1,\ldots, s_t=j,s_{t+1}=i, y_1,\ldots y_t, y_{t+1} \mid \lambda) &amp; = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\\&amp; \times P(s_{t+1}=i,y_{t+1} \mid s_1, \ldots s_t, y_1, \ldots y_t, \lambda) \\
 &amp; = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot P(s_{t+1} \mid s_t, \lambda)\\ &amp; \times P(y_{t+1} \mid s_{t+1},\lambda) \\
  &amp; = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot a_{ij}b_i(y_{t+1})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus &lt;script type=&quot;math/tex&quot;&gt;\delta_{t+1}(i)&lt;/script&gt;  is computed recursively from &lt;script type=&quot;math/tex&quot;&gt;\delta_{t+1}(j)&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta_{t+1}(i) &amp;= \max_{s_1,\ldots s_{t}=j} P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot a_{ij}b_i(y_{t+1}) \\
 &amp; = \max_{j}\Big[ \delta_t(j) a_{ij}\Big]\cdot b_i(y_{t+1})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We therefore need to keep track the state that maximize the above equation so as to backtrack to the single best state sequence in the following Viterbi algorithm:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initilization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;1 \leq i \geq N&lt;/script&gt;, let:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta _1(i)&amp;= \pi _{s_i}b_i(y_1)\\
\Theta _1(i)&amp;=0
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Recursion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Calculate  the ML (maximum likelihood) state sequences and their probabilities. For &lt;script type=&quot;math/tex&quot;&gt;t=2,3,...T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1\leq i \geq N&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\delta_t(i) &amp; = \displaystyle \max_{j\epsilon{1,..N}} \Big[\delta_{t-1}(j)a_{ij}\Big]\cdot b_i(y_t) \\
\Theta_t(i) &amp; = \arg\max_j \Big[\delta_{t-1}(j)a_{ij} \Big] 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Termination&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Retrieve the most likely final state&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \hat{P} &amp;= \displaystyle \max_{j\epsilon{1,..N}}[\delta_T(j)]  \\
\hat{S}_T &amp; = \arg\max_j [\delta_T(j)]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;State sequence backtracking&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Retrieve  the most likely state sequences (virtebi path)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{S}_t = \Theta_{t+1}(\hat{S}_{t+1}) \text{, where } t=T-1,T-2,\ldots1&lt;/script&gt;

&lt;p&gt;Virtebi algorithm uses the same schema as the Forward algorithm except for two differences:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It uses maximization in place of summation at the recursion and termination steps.&lt;/li&gt;
  &lt;li&gt;It keeps track of the arguments that maximize &lt;script type=&quot;math/tex&quot;&gt;\delta_t(i)&lt;/script&gt; for each &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, storing them in the N by T matrix &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;. This matrix is used to retrieve the optimal state sequence at the backtracking step.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Python implementation of virtebi algorithm&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;viterbi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# returns the most likely state sequence given observed sequence x&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# using the Viterbi algorithm&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;psi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# backtrack&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To summarize, we can compute the following from HMM:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The marginalized likelihood function &lt;script type=&quot;math/tex&quot;&gt;P(Y \mid \lambda)&lt;/script&gt; from the forward or backward algorithm.&lt;/li&gt;
  &lt;li&gt;The posterior probability &lt;script type=&quot;math/tex&quot;&gt;\gamma_t(i) = P(s_t=i  \mid Y, \lambda)&lt;/script&gt; from the forward–backward algorithm.&lt;/li&gt;
  &lt;li&gt;The optimal state sequence &lt;script type=&quot;math/tex&quot;&gt;\hat{S} = \max_{s} P(S \mid Y, \lambda) = \max_{s} P(S, Y \mid  \lambda)&lt;/script&gt;from the Viterbi algorithm.&lt;/li&gt;
  &lt;li&gt;The segmental joint likelihood function &lt;script type=&quot;math/tex&quot;&gt;P(\hat{S},Y \mid \lambda)&lt;/script&gt; from the Viterbi algorithm.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These values are used in the decoding step and the training step of estimating model parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Conside the Bob-Alice example as described &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example&quot;&gt;here&lt;/a&gt;. Two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.&lt;/p&gt;

&lt;p&gt;Alice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his activities, those are the observations.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Rainy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Sunny'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'walk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'shop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'clean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#initial probability &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Transmission probability &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Emission probability&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Suppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bob_says&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alice_hears&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;viterbi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bob_says&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Bob says:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;observ_bob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bob_says&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Alice hears:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states_bob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alice_hears&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
    ('Bob says:', 'walk, clean, shop, shop, clean, walk')
    ('Alice hears:', 'Sunny, Rainy, Rainy, Rainy, Rainy, Sunny')
&lt;/blockquote&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;L. R. Rabiner, &lt;a href=&quot;http://www.cs.ucsb.edu/~cs281b/papers/HMMs%20-%20Rabiner.pdf&quot;&gt;A tutorial on hidden Markov models and selected applications in speech recognition&lt;/a&gt;, Proceedings of the IEEE, Vol. 77, No. 2, February 1989.&lt;/li&gt;
  &lt;li&gt;Shinji Watanabe, Jen-Tzung Chien, &lt;a href=&quot;https://books.google.co.tz/books/about/Bayesian_Speech_and_Language_Processing.html?id=rEzzCQAAQBAJ&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button&amp;amp;redir_esc=y#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;Bayesian Speech and Language Processing&lt;/a&gt;, Cambridge University Press, 2015.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/&quot;&gt;Viterbi Algorithm in Speech Enhancement and HMM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Nikolai Shokhirev, &lt;a href=&quot;http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html&quot;&gt;Hidden Markov Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Mixture models</title><link href="https://sambaiga.github.io/ml/2017/04/28/mixture-models.html" rel="alternate" type="text/html" title="Mixture models" /><published>2017-04-28T17:12:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/ml/2017/04/28/mixture-models</id><content type="html" xml:base="https://sambaiga.github.io/ml/2017/04/28/mixture-models.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The previous post presented different approaches for estimating parameter of probabilistic models with single mode. However, in practise the data we are trying to model is much complex with more than one unknown or unobservable
quantity. For instance consider a case when we have collected the aggregate power consumption of a builings over the course of month and we want to decompose this signal into a sum of components which match to various appliances in the buildings. In this case we must model the data in terms of mixture of several components in which each component has a simple parametric form. We will assume each data point belongs to one of the components and try to infer the distribution for each component separately.&lt;/p&gt;

&lt;h2 id=&quot;mixture-models&quot;&gt;Mixture Models&lt;/h2&gt;

&lt;p&gt;A mixture model is a probabilistic model for representing the presence of components (subpopulation) within an overal population (data). It often used to learn probabilistic models for unspervised learning problems (clustering). To formulate mixture model mathematically we introduce a &lt;strong&gt;latent variable&lt;/strong&gt; denoted as &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt;. The latent variables are the hidden units by which the algorithms need to figure. It correspond to a mixture components and is represented by a discrete state &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}_i=[1 \ldots K]&lt;/script&gt;. The variables which are alwalys observed are knowas &lt;strong&gt;observables&lt;/strong&gt;. In the above example, the power of each appliance is the latent variable and the aggregate power is the observable variable.&lt;/p&gt;

&lt;h3 id=&quot;ingridients-of-mixture-models&quot;&gt;Ingridients of Mixture Models&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} = [x_1 \ldots x_n]&lt;/script&gt; denote the observation and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}=[z_1 \ldots z_k]&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;z_i \in [1\ldots K]&lt;/script&gt; be the latent vectors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The ditribution of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt;&lt;/strong&gt; given by &lt;script type=&quot;math/tex&quot;&gt;p(z_i=k)=\pi_k&lt;/script&gt; such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k=1}^K \pi _k =1&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;z_i&lt;/script&gt; assumed to be independent and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is the number of mixture component. This distribution is usually multinomial distribution denoted as:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{\pi} \sim Multinomial(\pi _1 \ldots \pi _k)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The likelihood distribution&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;P(x_i \mid z_i=k)&lt;/script&gt; which can take variety of parametric form though in this post we will assume it is gaussian distribution denoted as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x|z}\sim \mathcal{N}(\mu , \sigma)&lt;/script&gt;

&lt;p&gt;It is also knows base distribution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The overall mixture model&lt;/strong&gt; which is the distribution of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P(\mathbf{x}| \theta) &amp;= \sum_{k = 1}^K P(z_i = k)P(x_i|z_i = k) \\
 &amp;= \sum_{k = 1}^K \pi _k p(x_i|z_i = k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Posterior inference&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;P(z \mid \mathbf{x})&lt;/script&gt; in which given a data point &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and the known model parameter $\theta$ which component &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is likely to belong to. It is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(z \mid \mathbf{x}) \propto P(z)P(\mathbf{x}|z)&lt;/script&gt;

&lt;h3 id=&quot;application-of-mixture-models&quot;&gt;Application of Mixture Models&lt;/h3&gt;

&lt;p&gt;There are two main applications of mixture models namely:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Black-box density model which is useful in variety of tasks such as data compression, outlier detection and for creating generative classifiers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Clustering&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;gaussian-mixture-model&quot;&gt;Gaussian Mixture Model&lt;/h2&gt;

&lt;p&gt;The most widely used mixture model is the gaussian mixture model (GMM). In this model the likelihood (base) distribution in the mixture is a multivariate Gaussian with mean &lt;script type=&quot;math/tex&quot;&gt;\mu _k&lt;/script&gt; and covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\Sigma _k&lt;/script&gt; . Thus the model has the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathbf{x}|\theta) = \sum_{k=1}^K \pi _k \mathcal{N}(\mathbf{x}|\mu _k, \Sigma _k)&lt;/script&gt;

&lt;h3 id=&quot;univariate-gausian-mixture-model-example&quot;&gt;Univariate Gausian Mixture Model Example&lt;/h3&gt;

&lt;p&gt;Let generate a univariate gausian mixture model with two component in which we choose component 1 with probability &lt;script type=&quot;math/tex&quot;&gt;0.7&lt;/script&gt; otherwise we choose component 2. If we choose the first component we then sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from a gausian with mean 0 and standard deviation 1. And if we choose component 2, we sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from a gausian distribution with mean 6 and standard deviation 2. Let implent this in python as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Import important  modules&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'figure.figsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# set default size of plots&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image.cmap'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;k_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The first component&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;k_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The second component&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# define x random variable from -5 to 12&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mixture_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#let plot our model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;GMM&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title_font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PDF&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis_font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;random variable x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis_font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Component 1&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;g&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Component 2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mixture_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Mixture&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fancybox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shadow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/img/post/mixture-1.png&quot; title=&quot;Univariate mixture of gaussian model&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;Univariate mixture of gaussian model
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;learning-the-parameters-of-a-gaussian-mixture-model&quot;&gt;Learning the Parameters of a Gaussian Mixture Model&lt;/h3&gt;

&lt;p&gt;Specifically we need two set of parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The mean &lt;script type=&quot;math/tex&quot;&gt;\mu _k&lt;/script&gt; and covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\Sigma _k&lt;/script&gt; associated with each component &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The mixing proportions &lt;script type=&quot;math/tex&quot;&gt;\pi _k&lt;/script&gt; defined as &lt;script type=&quot;math/tex&quot;&gt;P(z_i =k)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before we proceed we should note that the log-likelihood derivative of a distribution with respect to parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is given as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d}{d\theta} \log P(\mathbf{x}\mid \theta) &amp; =\sum_z P(z\mid \mathbf{x}) \frac{d}{d\theta} \log P(z,\mathbf{x}) \\
 &amp; = \mathbb{E}_{p\mid \mathbf{x}}\left[ \frac{d}{d\theta} \log P(z,\mathbf{x}) \right]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is simply the expected derivative of the joint log-probability. Now if we consider the log-likelihood gradient of the univariate gaussian mixture model example presented above.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial}{\partial \mu _1}\log P(x) &amp; = \mathbb{E} _{P(z\mid x)} \left[\frac{\partial}{\partial \mu _1}\log P(z) + \frac{\partial}{\partial \mu _1}\log P(x\mid z)\right] \\
&amp; = P(z=1\mid x)\left[ \frac{\partial}{\partial \mu _1}\log P(z=1)+ \frac{\partial}{\partial \mu _1}\log P(x\mid z=1)\right]  \\&amp; + P(z=2\mid x)\left[ \frac{\partial}{\partial \mu _1}\log P(z=2)+ \frac{\partial}{\partial \mu _1}\log P(x\mid z=2)\right] \\
&amp; = P(z=1\mid x)\frac{\partial}{\partial \mu _1}\log P(x\mid z) \\
&amp; = P(z=1\mid x)\frac{\partial}{\partial \mu _1}\log \mathcal{N}(x\mid \mu _1, \sigma _1) \\
 &amp;=P(z=1\mid x)\frac{x-\mu _1}{\sigma _1^2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is because all of the terms except  &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial}{\partial \mu _1}\log P(x\mid z)&lt;/script&gt; are zero. Taking the sum of the above formula over all training example &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \mathcal{L}}{\partial \mu _1} = \sum _{i=1}^N P(z^{(i)}=1\mid x^{(i)})\frac{x^{(i)}-\mu _1}{\sigma _1^2}&lt;/script&gt;

&lt;p&gt;Let introduce &lt;script type=&quot;math/tex&quot;&gt;r_k^{(i)} = P(z^{(i)}=1\mid x^{(i)})&lt;/script&gt; which are called responsibilities as they say how strongly a data point belong to each component. The above equation become.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \mathcal{L}}{\partial \mu _1} = \sum _{i=1}^N r_k^{(i)}\frac{x^{(i)}-\mu _1}{\sigma _1^2}&lt;/script&gt;

&lt;p&gt;However we can not just solve for &lt;script type=&quot;math/tex&quot;&gt;\mu _1&lt;/script&gt; in the equation above because the responsibility &lt;script type=&quot;math/tex&quot;&gt;r_k^{(i)}&lt;/script&gt; depend on &lt;script type=&quot;math/tex&quot;&gt;\mu _1&lt;/script&gt;. This is the basis of the expectationmaximazation lagorithm  discussed below.&lt;/p&gt;

&lt;h4 id=&quot;expectation-maximazation-em&quot;&gt;Expectation Maximazation (EM)&lt;/h4&gt;

&lt;p&gt;EM is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. The algorithm gets its name because it involves two important steps: computing the responsibilities and applying the maximum likelihood update with those responsibilities as follows:&lt;/p&gt;

&lt;p&gt;Repeat until converged:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E-step&lt;/strong&gt;: Compute the expectations or responsibilities of the latent variables&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_k^{(i)} \leftarrow P(z^{(i)}=k\mid x^{(i)})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;M-step&lt;/strong&gt;: Compute the maximum likelihood parameters given these expectations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\theta} \leftarrow \underset{\mathbf{\theta}}{\operatorname{argmax}} \sum_{i=1}^N \sum_{k=1}^K r_k^{(i)} \left[\log P(z^{(i)}=k) + \log P(\mathbf{x}^{(i)}\mid z^{(i)}=k)  \right]&lt;/script&gt;

&lt;h3 id=&quot;em-for-gaussian-mixture-model&quot;&gt;EM for Gaussian Mixture Model&lt;/h3&gt;

&lt;p&gt;We first need to &lt;strong&gt;initilize the parameter&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu _k}, \hat{\sigma _k}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi _k}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expectation Step&lt;/strong&gt;: compute the responsibilities
 Hi elom&lt;/p&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Learning Probabilistic Models</title><link href="https://sambaiga.github.io/ml/hmm/2017/04/25/probabilistic-models.html" rel="alternate" type="text/html" title="Learning Probabilistic Models" /><published>2017-04-25T17:12:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/ml/hmm/2017/04/25/probabilistic-models</id><content type="html" xml:base="https://sambaiga.github.io/ml/hmm/2017/04/25/probabilistic-models.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Given some data &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}=[x_1\ldots x_m]&lt;/script&gt;  that come from some probability density function characterized by
an unknown parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; . How can we find &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;  that is the best estimator of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. For example suppose we have flipped a particular coin &lt;script type=&quot;math/tex&quot;&gt;100&lt;/script&gt;  times and landed head &lt;script type=&quot;math/tex&quot;&gt;N_H = 55&lt;/script&gt;  times and tails &lt;script type=&quot;math/tex&quot;&gt;N_T = 45&lt;/script&gt;  times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behaviour of the coin can be summerized with parameter  &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;  the probability that a flip land head (H) which in this case is independent and identically ditributed Bernoulli distribution. The key question is how do we find parameter  &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;  of this distribution that fit the data. This is called parameter estimation in which three approaches can be used:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Maximum-Likehood estimation&lt;/li&gt;
  &lt;li&gt;Bayesian parameter estimation and&lt;/li&gt;
  &lt;li&gt;Maximum a-posterior approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;like-hood-and-log-likehood-function&quot;&gt;Like-hood and log-likehood function&lt;/h3&gt;

&lt;p&gt;Let firts define the &lt;strong&gt;like-hood function&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;L(\theta)&lt;/script&gt; which is the probability of the observed data as function of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; given as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = P(x_1,\ldots x_m; \theta) = \prod_i^m P(x_i;\theta)&lt;/script&gt;

&lt;p&gt;The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \theta ^{N_H}(1 - \theta ^{N_T})&lt;/script&gt;

&lt;p&gt;We also define the &lt;strong&gt;log-likelihood function&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\theta)&lt;/script&gt; which is the log of the likelihood function &lt;script type=&quot;math/tex&quot;&gt;L(\theta)&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}(\theta) &amp;= \log L(\theta) \\
 &amp; = \log \prod_i^m P(x_i;\theta) \\
  &amp; = \sum_i^M P(x_i;\theta)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the above coin example the log-likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\theta)= N_H\log\theta + N_T\log(1-\theta)&lt;/script&gt;

&lt;h2 id=&quot;maximum-likelihood-estimation&quot;&gt;Maximum-Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;The main objective of maximum likelihood estimation (MLE) is to determine the value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; that is most likely to have generated the vector of observed data, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is assumed to be  fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt; is selected such that it maximize &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\theta)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}=\arg\max_{\theta} \mathcal{L}(\theta)&lt;/script&gt;

&lt;p&gt;For the coin example the MLE is :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} &amp; = \frac{\partial }{\partial \theta}(N_H\log\theta + N_T\log(1-\theta) \\
 &amp;= \frac{N_H}{\theta} - \frac{N_T}{1-\theta}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Set &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 0&lt;/script&gt;  and  solve for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; we obtain the MLE:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} =\frac{N_H}{N_H + N_T}&lt;/script&gt;

&lt;p&gt;which is simply the fraction of flips that cameup head.&lt;/p&gt;

&lt;p&gt;Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and standard deviation &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;. We can use MLE to estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{\sigma}&lt;/script&gt;. The log-likehood for gausian distribution is given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}(\theta) &amp;= \sum_{i=1}^M \log \left[ \frac{1}{\sqrt{2} \pi \sigma} \exp \frac{-(x_i - \mu)}{2\sigma ^2}\right] \\
 &amp; = -\frac{M}{2}\log 2\pi - M\log \sigma - \frac{1}{2\sigma^2} \sum_i^M (x_i - \mu)^2
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let find &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}(\theta)}{\partial \mu}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}(\theta)}{\partial \sigma}&lt;/script&gt; and set  equal to zero.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \mu} &amp;=  -\frac{1}{2\sigma^2} \sum_i^M \frac{\partial}{\partial \mu}(x_i - \mu)^2 \\
&amp; = \sum_i^M (x_i - \mu) = 0 \\
&amp;\Rightarrow \hat{\mu} = \frac{1}{M} \sum_{i=1}^M x_i
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is the mean of the observed values.&lt;/p&gt;

&lt;p&gt;Similary:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \sigma} &amp;=  \frac{M}{\sigma} + \frac{1}{\sigma^3}\sum_i^M (x_i - \mu)^2 \\
&amp;\Rightarrow \hat{\sigma} = \sqrt{\frac{1}{M} \sum_{i=1}^M (x_i - \mu)^2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the two examples above  we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let’s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x) = \frac{b^a}{\Gamma(a)}x^{x-1}\exp(-bx)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Gamma (a)&lt;/script&gt; is the gamma function which is the generalization of the factorial function to continous values given as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Gamma(t) = \int_0^{-\infty} x^{t-1}\exp(-x) \,dx&lt;/script&gt;

&lt;p&gt;The model parameters for gamma distribution is &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; both of which are &lt;script type=&quot;math/tex&quot;&gt;\geq 0&lt;/script&gt;. the log-likelihood is therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \mathcal{L}( a, b) &amp; = \sum_{i=1}^M a\log b -\log \Gamma (a) + (a -1) \log x_i - bx_i \\
 &amp; = Ma\log b - M \log \Gamma (a) + (a - 1) \sum_{i=1}^M \log x_i - b \sum_{i=1}^M x_i
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;To get MLE we need employ gradient descent which consists of computing the derivatives:
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}}{\partial a}&lt;/script&gt; and 
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}}{\partial b}&lt;/script&gt; and then updating;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_{k+1}= a_k + \alpha \frac{\partial \mathcal{L}}{\partial a}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_{k+1}= b_k + \alpha \frac{\partial \mathcal{L}}{\partial b}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the learning rate.&lt;/p&gt;

&lt;h3 id=&quot;limitation-of-mle&quot;&gt;Limitation of MLE&lt;/h3&gt;

&lt;p&gt;Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped  a coin twice and &lt;script type=&quot;math/tex&quot;&gt;N_H = 2&lt;/script&gt;, the MLE of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, the probability of H would be &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. This imply that we are considering it impossible for the coin to come up T. This problem is knowas &lt;em&gt;data sparsity&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-parameter-estimation&quot;&gt;Bayesian Parameter Estimation&lt;/h2&gt;

&lt;p&gt;Unlike MLE which treat only the observation &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; as random variable and the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; as a fixed point, the bayesian approach treat the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; as random varibale as well with some known prior distribution. Let define the model for joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(\theta, \mathcal{D})&lt;/script&gt; over parameter  &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and data &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;. To further define this joint distribution we aslo need the following two distribution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A distribution of &lt;script type=&quot;math/tex&quot;&gt;P(\theta)&lt;/script&gt; knowas &lt;strong&gt;prior distribution&lt;/strong&gt; which is the probability of paratemeter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; before observing the data. In practise choose prior that is computational convinient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;likelihood&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;P(\mathcal{D}\mid \theta)&lt;/script&gt; which is the probability of data given the parameter like in maximum likelihood.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution &lt;script type=&quot;math/tex&quot;&gt;P(\theta \mid \mathcal{D})&lt;/script&gt; which correspond to uncertainty about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; after observing the data given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P(\theta  \mid  \mathcal{D}) &amp;= \frac{P(\theta)p(\mathcal{D}  \mid  \theta)}{P(\mathcal{D})} &amp;= \frac{P(\theta)P(\mathcal{D} \mid  \theta)}{ \displaystyle \int P(\theta ^ {\prime} ) P(\mathcal{D} \mid  \theta ^{\prime})}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The denominator is usually considered as a normalizing constant and thus the posterior distribution become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta  \mid  \mathcal{D}) \propto P(\theta)P(\mathcal{D}  \mid \theta)&lt;/script&gt;

&lt;p&gt;On the other hand the posterior predictive distribution &lt;script type=&quot;math/tex&quot;&gt;P(\mathcal{D}^{\prime}\mid)\mathcal{D}&lt;/script&gt; is the distribution of future observation given past observation defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathcal{D}^{\prime} \mid \mathcal{D} )= \int P(\theta\mid \mathcal{D}) P(\mathcal{D}^{\prime} \mid \theta)&lt;/script&gt;

&lt;p&gt;Generaly the Bayesian approach to parameter estimation works as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective
beliefs and subjective uncertainty about the unknown parameters, before seeing the data.&lt;/li&gt;
  &lt;li&gt;Gather data&lt;/li&gt;
  &lt;li&gt;Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown
parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to &lt;script type=&quot;math/tex&quot;&gt;\theta^{N_H}(1-\theta)^{N_T}&lt;/script&gt;. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies &lt;script type=&quot;math/tex&quot;&gt;p(\theta) = 0.5&lt;/script&gt;. We can also use various distribution to specify prior density but in practise a most useful distribution is the &lt;strong&gt;beta distribution&lt;/strong&gt; parameterized by &lt;script type=&quot;math/tex&quot;&gt;a , b &gt; 0&lt;/script&gt; and defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta; a, b) = \frac{\Gamma (a + b)}{\Gamma(a) \Gamma (b)} \theta ^{a-1}(1-\theta ^{b - 1})&lt;/script&gt;

&lt;p&gt;From the above eqution it is clear that the first term (with all $\Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta; a, b) \propto \theta ^{a-1}(1-\theta) ^{b - 1}&lt;/script&gt;

&lt;p&gt;Note the beta distribution has the following properties&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is centered around &lt;script type=&quot;math/tex&quot;&gt;\frac{a}{a + b}&lt;/script&gt; and it can be shown that if &lt;script type=&quot;math/tex&quot;&gt;\theta \sim \text{Beta}(a,b)&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}(\theta)=\frac{a}{a + b}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;It becomes more peaked for larger values of &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;It become normal distribution when &lt;script type=&quot;math/tex&quot;&gt;a = b = 1&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let compute the posterior and posterior predictive distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(\theta | \mathcal{D}) &amp; \propto p(\theta)p(\mathcal{D} |\theta) \\
&amp; \propto \theta^{N_H}(1-\theta)^{N_T}\theta ^{a-1}(1-\theta) ^{b - 1} \\
&amp; = \theta ^{a-1+N_H}(1-\theta) ^{b - 1 + N_T}
\end{aligned} %]]&gt;&lt;/script&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Introduction to Machine Learning - Classification.</title><link href="https://sambaiga.github.io/ml/2017/04/15/ml-classification.html" rel="alternate" type="text/html" title="Introduction to Machine Learning - Classification." /><published>2017-04-15T17:12:00+02:00</published><updated>2017-10-01T19:21:51+02:00</updated><id>https://sambaiga.github.io/ml/2017/04/15/ml-classification</id><content type="html" xml:base="https://sambaiga.github.io/ml/2017/04/15/ml-classification.html">&lt;p&gt;Previously we learned how to predict continuous-valued quantities as a linear function of input values.This post will describe a classification probem where the goal is to learn a mapping from inputs &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; to target &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;t \in \{1\ldots C \}&lt;/script&gt; with with &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; being the number of classes.If &lt;script type=&quot;math/tex&quot;&gt;C = 2&lt;/script&gt;, this is called binary classification (in which case we often assume &lt;script type=&quot;math/tex&quot;&gt;y \in \{0, 1\}&lt;/script&gt;; if &lt;script type=&quot;math/tex&quot;&gt;C &gt; 2&lt;/script&gt;, this is called multiclass classification.&lt;/p&gt;

&lt;p&gt;We will first consider binary classification problem in which the target classes &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; will be generated from 2 class distributions: blue (&lt;script type=&quot;math/tex&quot;&gt;t=1&lt;/script&gt;) and red (&lt;script type=&quot;math/tex&quot;&gt;t=0&lt;/script&gt;). Samples from both classes are sampled from their respective distributions. These samples are plotted in the figure below.&lt;/p&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;N \times 2&lt;/script&gt; matrix of individual input samples &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i&lt;/script&gt;, and that &lt;script type=&quot;math/tex&quot;&gt;\mathbf{t}&lt;/script&gt; is a corresponding &lt;script type=&quot;math/tex&quot;&gt;N \times 1&lt;/script&gt; vector of target values &lt;script type=&quot;math/tex&quot;&gt;t_i&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;With logistic regression the goal is to predict the target class &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; from the input values &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. The network is defined as having an input &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} = [x_1, x_2]&lt;/script&gt; which gets transformed by the weights &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w} = [w_1, w_2]&lt;/script&gt; to generate the probability that sample &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; belongs to class &lt;script type=&quot;math/tex&quot;&gt;t=1&lt;/script&gt;. This probability &lt;script type=&quot;math/tex&quot;&gt;P(t=1\mid \mathbf{x},\mathbf{w})&lt;/script&gt; is represented by the output &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; of the network computed as &lt;script type=&quot;math/tex&quot;&gt;y = \sigma(\mathbf{x} * \mathbf{w}^T)&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt; and is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma(z) = \frac{1}{1+e^{-z}}&lt;/script&gt;

&lt;p&gt;which squashes the predictions to be between 0 and 1 such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P(t=1| \mathbf{x},\mathbf{w}) &amp;= y(\sigma(z))P(t=0\mid \mathbf{x},\mathbf{w})\\
 &amp;= 1 - P(t=1\mid \mathbf{x},\mathbf{w}) = 1 - y(\sigma(z))
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The loss function for logistic function is called crossentropy and defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{CE}(y,t)=\begin{cases} -\log y \quad \text{if } t = 1\\ -\log (1-y) \quad \text{if } t = 0
\end{cases}&lt;/script&gt;

&lt;p&gt;The crossentropy can be written in other form as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{CE}(y,t)= -t \log y -(1-t)\log(1-y)&lt;/script&gt;

&lt;p&gt;When we combine the logistic activation function with cross-entropy loss, we get logistic regression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
z &amp; = \mathbf{w^Tx + b}\\\ y &amp; = \sigma(z)\\\ \mathcal{L}_{CE}(y,t) &amp;= -t \log y -(1-t)\log(1-y)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The cost function with respect to the model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (i.e. the weights and bias) is therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\varepsilon_{\theta} &amp; = \frac{1}{N}\sum_{i=1}^N \mathcal{L}_{CE}(y,t)\\\ &amp; = \frac{1}{N}\sum_{i=1}^N \left(-t^{(i)} \log y^{(i)} -(1-t^{(i)})\log(1-y^{(i)})\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which can be implemented in python as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define the cost function&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;gradient-descent-for-logistic-function&quot;&gt;Gradient Descent for Logistic Function&lt;/h3&gt;

&lt;p&gt;To derive the gradient descent updates, we’ll need the partial derivatives of the cost function. We’ll do this by applying the Chain Rule twice: first to compute 
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}_{CE}}{\partial z}&lt;/script&gt; 
and then again to compute &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}_{CE}}{\partial w_j}&lt;/script&gt; But first, let’s find 
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial y}{\partial z}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial y}{ \partial z}  = \frac{e^{-z}}{(1 + e^{-z})^2}= y(1-y)&lt;/script&gt;

&lt;p&gt;Now for the Chain Rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}_{CE}}{\partial z} &amp; =\frac{\partial \mathcal{L}_{CE}}{\partial y}\frac{\partial y}{ \partial z}\\\ &amp; = \left(\frac{-t}{y} + \frac{1-t}{1-y}  \right) y(1-y)\\\ &amp;= y - t
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similary:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}_{CE}}{\partial w_j} &amp; =\frac{\partial \mathcal{L}_{CE}}{\partial z}\frac{\partial z}{ \partial w_j}\\\ &amp;  =\frac{\partial \mathcal{L}_{CE}}{\partial z} x_j\\\ &amp;= (y - t)x_j
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can also obtain &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathcal{L}_{CE}}{\partial b}&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}_{CE}}{\partial b} &amp;= \frac{\partial \mathcal{L}_{CE}}{\partial z}\frac{\partial z}{\partial b}\\ &amp; = (y-t)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The gradient descent algorithm works by taking the derivative of the cost function &lt;script type=&quot;math/tex&quot;&gt;\varepsilon_{\theta}&lt;/script&gt; with respect to the parameters, and updates the parameters in the direction of the negative gradient.The parameter &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; is iteratively updated by taking steps proportional to the negative of the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w_{k+1}} = \mathbf{ w_k }- \alpha \frac{\partial \mathbf{\varepsilon}}{\partial \mathbf{w}}&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}_{CE}}{\partial \varepsilon} &amp;= \frac{\partial \varepsilon }{\partial \mathcal{L}_{CE}}\cdot\frac{\partial \mathcal{L}_{CE}}{\partial \mathbf{w}}\\ &amp;= \frac{1}{N} \mathbf{x^T(y - t)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which can be implemented in python as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#gradient&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;solve_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w_cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Stopping Condition&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Converged.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Iteration: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d - cost: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.4&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let us apply the above concept in the following example. Consider the case we want to predict whether a student with certain pass mark can be admitted or not.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# load dataset&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;admission&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/admission.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;grade1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;grade2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;remark&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;admission&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The data-preprosessing is done using the following python code:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'grade1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'grade2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'remark'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;targetVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;admission&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;admission&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targetVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Standardize the features&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Add bias term to feature data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;featureVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# randomly separate data into training and test data&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We use the solve_gradient function defined before to find the parameter for logistic regression&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;w_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solve_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now that you learned the parameters of the model, you can use the model to predict whether a particular student will be admited.&lt;/p&gt;

&lt;p&gt;Let define the  prediction function that only  1 or 0 depending on the predicted class&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;around&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To find the accuracy of the model:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;100.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;100.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;After running the above codes we found that our model achieve  a training accuracy of &lt;script type=&quot;math/tex&quot;&gt;91.25&lt;/script&gt; and a test accuracy of &lt;script type=&quot;math/tex&quot;&gt;85&lt;/script&gt; percents.&lt;/p&gt;

&lt;h2 id=&quot;multiclass-classification&quot;&gt;Multiclass classification&lt;/h2&gt;

&lt;p&gt;So far we’ve talked about binary classification, but most classifcation problems involve more than two categories. Fortunately, this doesn’t require any new ideas: everything pretty much works by analogy with the binary
case. The first question is how to represent the targets. We could represent them as integers, but it’s more convenient to use indicator vectors, or a one-of-K encoding.&lt;/p&gt;

&lt;p&gt;Since there are &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; outputs and &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; inputs, the linear function requires &lt;script type=&quot;math/tex&quot;&gt;K \times D&lt;/script&gt; matrix as well as &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; dimensional bias vector. We use &lt;strong&gt;softmax function&lt;/strong&gt; which is the multivariate generalization given as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_k =  softmax(z_1 \ldots z_k) = \frac{e^{z_k}}{\sum_k e^{z_k}}&lt;/script&gt;

&lt;p&gt;and can be implented in python as&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, the loss function (cross-entropy) for multiple-output case can be generalized as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}_{CE}(y,t) &amp;= -\sum_{k=1}^K t_k \log y_k\\ &amp;= -\mathbf{t^T}\log\mathbf{y}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Combining these things together, we get multiclass logistic regression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
\mathbf{z} &amp;= \mathbf{wx + b} \\ \mathbf{y} &amp;= softmax(\mathbf{z})\\ \mathcal{L}_{CE}(y,t) &amp;=-\mathbf{t^T}\log\mathbf{y} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;gradient-descent-for-multiclass-logisitc-regression-for-multiclass-logistic-regression&quot;&gt;Gradient Descent for Multiclass Logisitc Regression for Multiclass logistic regression:&lt;/h2&gt;

&lt;p&gt;Let consider the derivative with respect to the loss:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial {\mathcal L}_\text{CE}}{\partial w_{kj}} &amp;= \frac{\partial }{\partial w_{kj}} \left(-\sum_l t_l \log(y_l)\right) \\ &amp;= -\sum_l \frac{t_l}{y_l} \frac{\partial y_l}{\partial w_{kj}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Normally in calculus we have the rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial y_l}{\partial w_{kj}} &amp;= \sum_m \frac{\partial y_l}{\partial z_m} \frac{\partial z_m}{\partial w_{kj}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;But &lt;script type=&quot;math/tex&quot;&gt;w_{kj}&lt;/script&gt; is independent of &lt;script type=&quot;math/tex&quot;&gt;z_m&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;m \ne k&lt;/script&gt;, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial y_l}{\partial w_{kj}} &amp;= \frac{\partial y_l}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;AND&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial z_k}{\partial w_{kj}} = x_j&lt;/script&gt;

&lt;p&gt;Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial {\mathcal L}_\text{CE}}{\partial w_{kj}} &amp;=  -\sum_l \frac{t_l}{y_l} \frac{\partial y_l}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}} \\
 &amp;= -\sum_l \frac{t_l}{y_l} \frac{\partial y_l}{\partial z_k} x_j \\
  &amp;= x_j (-\sum_l \frac{t_l}{y_l} \frac{\partial y_l}{\partial z_k}) \\
   &amp;= x_j \frac{\partial {\mathcal L}_\text{CE}}{\partial z_k} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now consider derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; we can show (on board) that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial y_l}{\partial z_k} = y_k (I_{k,l} - y_l)&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;I_{k,l} = 1&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;k=l&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; otherwise.&lt;/p&gt;

&lt;p&gt;Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial {\mathcal L}_\text{CE}}{\partial z_k} &amp;= -\sum_l \frac{t_l}{y_l} (y_k (I_{k,l} - y_l)) \\ &amp;=-\frac{t_k}{y_k} y_k(1 - y_k) - \sum_{l \ne k} \frac{t_l}{y_l} (-y_k y_l) \\
 &amp;= - t_k(1 - y_k) + \sum_{l \ne k} t_l y_k \\
  &amp;= -t_k + t_k y_k + \sum_{l \ne k} t_l y_k \\
   &amp;= -t_k + \sum_{l} t_l y_k \\
    &amp;= -t_k + y_k \sum_{l} t_l  \\
     &amp;= -t_k + y_k \\
      &amp;= y_k - t_k
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting it all together&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial {\mathcal L}_\text{CE}}{\partial w_{kj}} &amp;= x_j (y_k - t_k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In vectorization form it become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\frac{\partial \mathcal {L}_{CE}}{\partial {\mathbf W}} = (\mathbf{y} - \mathbf{t}) \mathbf{x}^T 
\end{aligned}&lt;/script&gt;

&lt;h3 id=&quot;cross-entropy-cost-function&quot;&gt;Cross-entropy cost function&lt;/h3&gt;

&lt;p&gt;The cross entropy cost function for multiclass classification is given with respect to the model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (i.e. the weights and bias) is therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\varepsilon_{\theta} &amp; = \frac{1}{N}\sum_{i=1}^N \mathcal{L}_{CE}(y,t)\\
 &amp; = \frac{-1}{N}\sum_{i=1}^N \sum_{k=1}^K t_k \log y_k
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The gradient descent algorithm will be:
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{w_{k+1}} = \mathbf{ w_k }- \alpha \frac{\partial \mathbf{\varepsilon}}{\partial \mathbf{w}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}_{CE}}{\partial \varepsilon} &amp;= \frac{\partial \varepsilon }{\partial \mathcal{L}_{CE}}\cdot\frac{\partial \mathcal{L}_{CE}}{\partial \mathbf{w}}\\
 &amp;= \frac{1}{N} \mathbf{x^T(y - t)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The jupyter notebook for this post can be found &lt;a href=&quot;https://github.com/sambaiga/PythonML/blob/master/MLwithPython/Classification%20.ipynb&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/&quot;&gt;CSC321 Intro to Neural Networks and Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/&quot;&gt;Supervised and Unsupervised Machine Learning Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Previously we learned how to predict continuous-valued quantities as a linear function of input values.This post will describe a classification probem where the goal is to learn a mapping from inputs to target such that with with being the number of classes.If , this is called binary classification (in which case we often assume ; if , this is called multiclass classification.</summary></entry><entry><title type="html">Introduction to Machine Learning</title><link href="https://sambaiga.github.io/ml/2017/04/12/ml-intro.html" rel="alternate" type="text/html" title="Introduction to Machine Learning" /><published>2017-04-12T17:12:00+02:00</published><updated>2017-10-16T07:43:12+02:00</updated><id>https://sambaiga.github.io/ml/2017/04/12/ml-intro</id><content type="html" xml:base="https://sambaiga.github.io/ml/2017/04/12/ml-intro.html">&lt;h2 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Machine learning is a set of algorithms that automatically detect patterns in data and use the uncovered pattern to make inferences or predictions. It is a subfield of artificial intelligence that aims to enable computers to learn on their own. Any machine learning algorithms involve the baisc three steps: first you identify pattern from data, then you build (train) model that best explain the pattern and the world (unseen data) and lastly use the model to predict or do inference. The process of training (building) a model can be seen as a learning process where the model is exposed to new, unfamiliar data step by step.&lt;/p&gt;

&lt;p&gt;Machine learning is an exciting and fast-moving field of computer science with many recent applications. Important applications where machine learning algorithms are regularly deployed includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computer vision: Object Classification in Photograph, &lt;a href=&quot;https://petapixel.com/2016/09/23/googles-image-captioning-ai-can-describe-photos-94-accuracy/&quot;&gt;image captioning&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Speech recognition, Automatic Machine Translation.&lt;/li&gt;
  &lt;li&gt;Detecting anomalies (e.g. Security, credit card fraud)&lt;/li&gt;
  &lt;li&gt;Speech recognition.&lt;/li&gt;
  &lt;li&gt;Communication systems&lt;sup&gt;&lt;a href=&quot;https://www.hhi.fraunhofer.de/en/departments/wn/research-groups/signal-and-information-processing/research-topics/machine-learning-and-data-mining-for-communication-systems.html&quot;&gt;ref&lt;/a&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Robots learning complex behaviors&lt;/li&gt;
  &lt;li&gt;Recommendations services like in Amazo or Netflix where intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch next&lt;sup&gt;&lt;a href=&quot;https://www.forbes.com/sites/bernardmarr/2016/09/30/what-are-the-top-10-use-cases-for-machine-learning-and-ai/#4f49a7d894c9&quot;&gt;ref&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Machine learning algorithms that learn to recognise what they see and hear are at the heart of Apple, Google, Amazon, Facebook, Netflix, Microsoft, etc.&lt;/p&gt;

&lt;h3 id=&quot;why-machine-learning&quot;&gt;Why Machine learning&lt;/h3&gt;

&lt;p&gt;For many problems such as recognizing people and objects and understanding human speech  it’s difcult to program the correct behavior by hand. However with machine learning these taks are easier. Other reasons we might want to use machine learning to solve a given problem:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A system might need to adapt to a changing environment. For instance, spammers are constantly trying to figure out ways to trick our e-mail spam classifers, so the classifcation algorithms will need to constantly adapt.&lt;/li&gt;
  &lt;li&gt;A learning algorithm might be able to perform better than its human programmers. Learning algorithms have become world champions at a variety of games, from checkers to chess to Go. This would be impossible if the programs were only doing what they were explicitly told to.&lt;/li&gt;
  &lt;li&gt;We may want an algorithm to behave autonomously for privacy or fairness reasons, such as with ranking search results or targeting ads.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;types-of-machine-learning&quot;&gt;Types of Machine Learning&lt;/h3&gt;

&lt;p&gt;Machine learning is usually divide into three  major  types: Supervised Learning, Unspervised Learning and&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;: Supervised learning is where you have input variables x and an output variable y and you use an algorithm to learn the mapping function from the input to the output&lt;sup&gt;&lt;a href=&quot;http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/&quot;&gt;ref&lt;/a&gt;&lt;/sup&gt;. For instance, if we’re trying to train a machine leearning algorithm to distinguish cars and trucks, we would collect images of cars and trucks, and label each one as a car or a truck. Supervised learning problems can be further grouped into regression and classification problems.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;A regression problem&lt;/strong&gt;: is when the output variable is a real value, such as “dollars” or “weight” e.g Linear regression and Random forest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;: A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease” e.g Support vector machines, Random forest and logistic regression.
 Some popular examples of supervised machine learning algorithms are:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Unspervised Learning&lt;/strong&gt; :Unsupervised learning is where you only have input data (X) and no corresponding output variables.We just have a bunch of data, and want to look for patterns in the data. For instance, maybe we have lots of examples of patients with autism, and want to identify different subtypes of the condition.The most important types of unsupervised learning includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Distribution modeling&lt;/strong&gt; where one has an unlabeled dataset (such as a collection of images or sentences), and the goal is to learn a probability distribution which matches the dataset as closely as possible.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt; where the aim is to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reiforcement Learning&lt;/strong&gt;: is &lt;a href=&quot;https://www.oreilly.com/ideas/reinforcement-learning-explained&quot;&gt;learning best actions based on reward or punishment&lt;/a&gt;. It involves learning what actions to take in a given situation, based on &lt;em&gt;rewards&lt;/em&gt;
and &lt;em&gt;penalties&lt;/em&gt;. Example a robot takes a big step forward, then falls. The next time, it takes a smaller step and is able to hold its balance. The robot tries variations like this many times; eventually, it learns the right size of steps to take and walks steadily. It has succeeded.&lt;/p&gt;

&lt;p&gt;There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. Action is what an agent can do in each state. When a robot takes an action in a state, it receives a reward, a feedback from the environment. A reward can be positive or negative (penalties).&lt;/p&gt;

&lt;h2 id=&quot;typical-ml-task-linear-regression&quot;&gt;Typical ML task: Linear Regression&lt;/h2&gt;

&lt;p&gt;In regression, we are interested in predicting a scalar-valued target, such as the price of a stock. By linear, we mean that the target must be predicted as a linear function of the inputs. This is a kind of supervised learning algorithm; recall that, in supervised learning, we have a collection of training examples labeled with the correct outputs. Example applications of linear regression include weather forecasting, house pricing prediction, student performance (GPA) prediction just to mention a few.&lt;/p&gt;

&lt;h3 id=&quot;linear-regression-formulating-a-learning-problem&quot;&gt;Linear Regression: Formulating a learning problem&lt;/h3&gt;
&lt;p&gt;In order to formulate a learning problem mathematically, we need to define two things: a &lt;em&gt;model (hypothesis)** and a *loss function&lt;/em&gt;. After defining model and loss function we solve an optimisation problem with the aim to find the model parameters that best fit the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model (Hypothesis)&lt;/strong&gt;: It is the set of allowable hypotheses, or functions that compute predictions from the inputs. In the case of linear regression, the model simply consists of linear functions given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \sum_j w_jx_j + b&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is the weights, and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is an intercept term, which we’ll call the bias. These two terms are called model parameters denoted as &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: It defines how well the model fit the data and thus show how far off the prediction &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is from the target &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and given as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L(y,t)} = \frac{1}{2}(y - t)^2&lt;/script&gt;

&lt;p&gt;Since the loss function show how far off the prediction is from the target for one data point. We also need to define a cost function. The cost function is simply the loss, averaged over all the training examples.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
J (w_1\ldots w_D,b) &amp; = \frac{1}{N} \sum_{i=1}^N \mathcal{L}(y^{(i)},t^{(i)}) \\
 &amp; = \frac{1}{2N}\sum_{i=1}^N (y^{(i)} - t^{(i)})^2 \\
 &amp;=\frac{1}{2N}\sum_{i=1}^N \left(\sum_j w_jx_j^{(i)} + b -t^{(i)} \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In vectorized form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{J} =\frac{1}{2N} \lVert\mathbf{y-t}\lVert^2 =\frac{1}{2N}\mathbf{(y - t)^T(y-t)} \quad \text{where}\quad \mathbf{y = w^Tx}&lt;/script&gt;

&lt;p&gt;The python implementation of the cost function (vectorized) is shown below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    Evaluate the cost function in a vectorized manner for 
    inputs `x` and targets `t`, at weights `w1`, `w2` and `b`.
    '''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Combine our model and loss function, we get an optimization problem, where we are trying to minimize a cost function with respect to the model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (i.e. the weights and bias).&lt;/p&gt;

&lt;h2 id=&quot;solving-the-optimization-problem&quot;&gt;Solving the optimization problem&lt;/h2&gt;
&lt;p&gt;We now want to find the choice of model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta _{w_1\ldots w_D,b}&lt;/script&gt; that minimizes &lt;script type=&quot;math/tex&quot;&gt;J (w_1\ldots w_D,b)&lt;/script&gt; as given in the cost function above.There are two methods which we can use: direct solution and gradient descent.&lt;/p&gt;

&lt;h3 id=&quot;direct-solution&quot;&gt;Direct Solution&lt;/h3&gt;
&lt;p&gt;One way to compute the minimum of a function is to set the partial derivatives to zero.For simplicity, let’s assume the model doesn’t have a bias term as shown in the equation below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_\theta =\frac{1}{2N}\sum_{i=1}^N \left(\sum_j w_jx_j^{(i)}  -t^{(i)} \right)&lt;/script&gt;

&lt;p&gt;In vectorized form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{J} =\frac{1}{2N}\lVert \mathbf{y-t}\rVert ^2 \frac{1}{2N}\mathbf{(y - t)^T(y-t)}  \quad \text{where}\quad \mathbf{y = wx}&lt;/script&gt;

&lt;p&gt;For matrix differentiation we need the following results:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \frac{\partial \mathbf{Ax}}{\partial \mathbf{x}} &amp; = \mathbf{A}^T \frac{\partial (\mathbf{x}^T\mathbf{Ax})}{\partial \mathbf{x}}\\ &amp; = 2\mathbf{A}^T\mathbf{x}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting the partial derivatives of cost function in vectorized form to zero we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\frac{\partial \mathbf{J}}{\partial \mathbf{w}} &amp; =\frac{1}{2N}\frac{\partial \left(\mathbf{w^Tx^Tx w} -2 \mathbf{t^Twx} + \mathbf{t^Tt}\right)}{\partial \mathbf{w}} \\
&amp;=\frac{1}{2N}\left(2\mathbf{x}^T\mathbf{xw} -2\mathbf{x}^T\mathbf{t}\right) \\
\mathbf{w} &amp;= (\mathbf{x^Tx})^{-1}\mathbf{x^Tt}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In python this result can be implemented as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;directMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    Solve linear regression exactly. (fully vectorized)
    
    Given `x` - NxD matrix of inputs
          `t` - target outputs
    Returns the optimal weights as a D-dimensional vector
    '''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;The optimization algorithm commonly used to train machine learning is the gradient descent algorithm. It works by taking the derivative of the cost function &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; with respect to the parameters at a specific position on this cost function, and updates the parameters in the direction of the negative gradient. The entries of the gradient vector are simply the partial derivatives with respect to each of the variables:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \mathbf{J}}{\partial \mathbf{w}} = \begin{pmatrix} \frac{\partial J}{\partial w_1}\\
 \vdots\\ \frac{\partial J}{\partial w_D}
\end{pmatrix}&lt;/script&gt;

&lt;p&gt;The parameter &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; is iteratively updated by taking steps proportional to the negative of the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w_{t+1}} = \mathbf{ w_t }- \alpha \frac{\partial \mathbf{J}}{\partial \mathbf{w}}  = \mathbf{w_t} - \mathbf{\frac{\alpha}{N}x^T(y-t)}&lt;/script&gt;

&lt;p&gt;In coordinate systems this is equivalent to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1} = w_t - \alpha \frac{1}{N}\sum_{i=1}^{N} x_t (y^{(i)}-t^{(i)})&lt;/script&gt;

&lt;p&gt;The python implementation of gradient descent is shown below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getGradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradientDescentMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#w = np.random.randn(D)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Perform Gradient Descent&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getGradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dw&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w_cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Stopping Condition&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Converged.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Iteration: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d - cost: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.4&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_cost&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;generalization&quot;&gt;Generalization&lt;/h2&gt;

&lt;p&gt;The goal of a learning algorithm is not to only to make correct predictions on the training examples; but it should be generalized to examples not seen seen before. The average squared error on novel examples is known as the generalization error, and we’d like this to be as small as possible. In practice, we nor- mally tune model parameters by partitioning the dataset into three different subsets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The training set is used to train the model.&lt;/li&gt;
  &lt;li&gt;The validation set is used to estimate the generalization error of each hyperparameter setting.&lt;/li&gt;
  &lt;li&gt;The test set is used at the very end, to estimate the generalization error of the final model, once all hyperparameters have been chosen.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Anthony Faustine</name></author><summary type="html">Machine Learning</summary></entry></feed>